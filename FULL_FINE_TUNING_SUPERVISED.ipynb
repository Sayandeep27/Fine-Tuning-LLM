{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised Fine-Tuning (SFT)**\n",
        "\n",
        "Supervised Fine-Tuning (SFT) is the process of further training a pre-trained model using a labeled dataset, where the input-output pairs (e.g., article â†’ summary) are known.\n",
        "\n"
      ],
      "metadata": {
        "id": "U-pGJhWDOV-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have done **full fine-tuning**, meaning all layers of the BART model were updated during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "71l0Lk2mOWLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full fine-tuning** means updating all the parameters (weights) of a pre-trained model on your new dataset. This gives the model full flexibility to adapt to your specific task, like summarization or classification. However, because large models like BART or T5 have hundreds of millions of parameters, full fine-tuning is very resource-intensive â€” it needs a lot of GPU memory, time, and storage. Itâ€™s best used when you have plenty of data and computing power.\n",
        "\n",
        "**PEFT (Parameter-Efficient Fine-Tuning)** is a smarter, lighter alternative where you freeze most of the model and train only a small number of added parameters, like **LoRA** adapters. This keeps the original model mostly unchanged but still allows it to learn new tasks efficiently. PEFT uses far less memory and is much faster to train, making it ideal for fine-tuning large models on limited hardware or smaller datasets."
      ],
      "metadata": {
        "id": "TrYG0PNMOWNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Full Fine Tuning of BART LLM**"
      ],
      "metadata": {
        "id": "5TRiaImqOWQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Required Libraries\n",
        "\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "\n",
        "#You install the datasets and transformers libraries needed for loading datasets and models."
      ],
      "metadata": {
        "id": "ZI2fC17IPovY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Dataset\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "#You load the CNN/DailyMail summarization dataset, which has article and highlights fields (text + summaries)."
      ],
      "metadata": {
        "id": "EdIMLcQUPoxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pre-trained Model and Tokenizer\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#You use the BART model, which is powerful for sequence-to-sequence tasks like summarization."
      ],
      "metadata": {
        "id": "v41lAsfLPo09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the Dataset\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [doc for doc in examples[\"article\"]]  # Input text\n",
        "    targets = [summary for summary in examples[\"highlights\"]]  # Summary\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize targets (summary) with the same settings\n",
        "    labels = tokenizer(targets, max_length=150, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "#You tokenize both the article (input) and highlights (target).\n",
        "#You pad/truncate both to fixed lengths.\n",
        "#You return inputs + labels for training."
      ],
      "metadata": {
        "id": "fcIXJ97gP-Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "76wLzTCOQOa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Training Arguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,  # ðŸš¨ Ensure this is set to False\n",
        "    report_to=\"none\"  # ðŸš¨ Disable logging to Hugging Face (prevents API key prompt)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "5It-uBlqQJLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the Model\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# You use Hugging Faceâ€™s Trainer API.\n",
        "# You fine-tune the BART model on the training set and validate on the validation set."
      ],
      "metadata": {
        "id": "oqab_QkyP-Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Fine-tuned Model\n",
        "\n",
        "model.save_pretrained(\"./fine_tuned_bart\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bart\")"
      ],
      "metadata": {
        "id": "EsO7Pik5P-SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Fine-tuned Model for Inference\n",
        "\n",
        "fine_tuned_pipe = pipeline(\"summarization\", model=\"./fine_tuned_bart\")\n",
        "summary = fine_tuned_pipe(text, max_length=100, min_length=30)\n",
        "\n",
        "# Load the model using Hugging Faceâ€™s pipeline.\n",
        "# Run a sample text through it to generate a summary.\n",
        "\n"
      ],
      "metadata": {
        "id": "IYqFIGbIP-UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LNipW_oNP-X4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}